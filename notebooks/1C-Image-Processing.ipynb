{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c10a4184-acdd-4d60-96d1-be3e7a9fbd21",
   "metadata": {},
   "source": [
    "# Image Processing\n",
    "\n",
    "Image processing = image in > image out\n",
    "Aims to suppress distortions and enhance relevant information\n",
    "Used to prepare images for further analysis and interpretation\n",
    "**Image analysis** = image in > features/measurements out\n",
    "**Computer vision** = image in > interpretation out\n",
    "\n",
    "## Types of image processing\n",
    "Two main types of operations:\n",
    "- Spatial domain operations (in image space)\n",
    "- Transform domain operations (mainly in Fourier space)\n",
    "\n",
    "Two main types of spacial domain operations:\n",
    "- Point operations (intensity transformations on individual pixels)\n",
    "- Neighbourhood operations (spatial filtering on groups of pixels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0713d6ee-8b78-408c-b5b4-13049b5cbf43",
   "metadata": {},
   "source": [
    "## Spatial domain operations\n",
    "\n",
    "General form of spatial domain operations\n",
    "$$g(x,y) = T[f(x,y)]$$\n",
    "where\n",
    "$f(x,y)$ - input image\n",
    "$g(x,y)$ - processed image\n",
    "$T[\\cdot]$ - operator applied at $(x,y)$\n",
    "\n",
    "Point operations: $T$ operates on individual pixels\n",
    "$$T: \\mathbb{R} -> \\mathbb{R} \\ \\ \\ \\ \\ \\ \\ \\ g(x,y) = T(f(x,y))$$\n",
    "\n",
    "Neighbourhood operations: $T$ operatoes on multiple pixels\n",
    "$$T: \\mathbb{R^n} -> \\mathbb{R} \\ \\ \\ \\ \\ \\ \\ \\ g(x,y) = T(f(x,y), f(x + 1, y), f(x -1, y), ...)$$\n",
    "<img src=\"../images/Neighbourhood-operations.png\" alt=\"Point operation\" align=\"center\">\n",
    "\n",
    "## Point operation\n",
    "<img src=\"../images/point-operations.png\" alt=\"Point operation\" align=\"center\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a62acc45-7338-44f3-bb55-6e591c78dee3",
   "metadata": {},
   "source": [
    "## Contrast stretching\n",
    "\n",
    "<img src=\"../images/constrast-streching.png\" alt=\"Contrast streching\" align=\"center\" >\n",
    "\n",
    "Produces images of higher constrast\n",
    "Puts values below $L$ in the input to the minimum (black) in the output\n",
    "Puts values above $H$ in the input to the maximum (white) in the output\n",
    "Linearly scale values between $L$ and $H$ (inclusive) in the input to between the minimum (black) and the maximum (white) in the output\n",
    "\n",
    "## Intensity thresholding\n",
    "<img src=\"../images/intensity-thresholding.png\" alt=\"Intensity thresholding\" align=\"center\" >\n",
    "\n",
    "Limiting case of contrast thresholding\n",
    "Produces binary images of gray-scale images\n",
    "Puts values below the threshold to black in the output\n",
    "Puts values equal/above the threshold to white in the output\n",
    "Useful only if object and background intensities are very different\n",
    "Result depends strongly on the threshold level (user params)\n",
    "\n",
    "## Automatic intensity thresholding\n",
    "Ostu's method for computing the threshold automatically\n",
    "Exhaustively searches for the threshold *minimising the intra-class variance*\n",
    "$$\\sigma^2_w = p_0\\sigma^2_0 + p_1\\sigma^2_1$$\n",
    "Equivalent to *maximising the inter-class variance* (much faster to compute)\n",
    "$$\\sigma^2_B = p_0p_1(\\mu_0 - \\mu_1)^2$$\n",
    "Here, $p_0$ is the fraction of pixels below the threshold (class 0), $p_1$ is the fraction of pixels equal to or above the threshold (class 1), $\\mu_0$ and $\\mu_1$ are the mean intensities of pixels in class 0 and class 1, $\\sigma^2_0$ and $\\sigma^2_1$ are the intensity variances, and $p_0 + p_1 = 1$\n",
    "\n",
    "**Ostu Example**\n",
    "<img src=\"../images/ostu-thresholding.png\" align=\"center\" >\n",
    "\n",
    "IsoData method for computing the threshold automatically\n",
    "1. Select an arbitrary initial threshold $t$\n",
    "2. Computer $\\mu_0$ and $\\mu_1$ with respect to the threshold\n",
    "3. Update the threshold to the mean of the means: $t = (\\mu_0 + \\mu_1) / 2$\n",
    "4. If the threshold changed in step 3, go to step 2\n",
    "Upon convergence, the threshold is midway between the two class means\n",
    "\n",
    "**IsoData thresholding example**\n",
    "<img src=\"../images/IsoData-thresholding.png\" align=\"center\">\n",
    "\n",
    "## Multilevel thresholding\n",
    "<img src=\"../images/multilevel-thresholding.png\" align=\"center\">\n",
    "\n",
    "## Intensity inversion\n",
    "<img src=\"../images/intensity-inversion.png\" align=\"center\">\n",
    "\n",
    "**Intensity inversion example**\n",
    "<img src=\"../images/intensity-inversion-example.png\" align=\"center\">\n",
    "Assessment of grayscale inverted images in addition to standard images facilitates the detection of microcalcification."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17782143-96ec-4edc-a61f-ea01ddffd881",
   "metadata": {},
   "source": [
    "## Log transformation\n",
    "Definition of log transformation\n",
    "$$s = clog(1+r)$$\n",
    "where $r$ is the input intensity, $s$ is the output intensity, and $c$ is a constant\n",
    "- Maps a narrow input range of low gray-level values into a wider range of output values, and vice versa for higher gray-level values\n",
    "- Also compresses the dynamic range of images with large variations in pixel values (such Fourier spectra)\n",
    "\n",
    "## Power transformation\n",
    "Definition of power transformation\n",
    "$$s = cr^y$$\n",
    "where $c$ and $y$ are constants\n",
    "- Similar to inverse log transformation\n",
    "- Represents a family of transformations by varying $y$\n",
    "- Many devices respond according to a power law\n",
    "- e.g. gamma correction\n",
    "- useful for general-purpose contrast manipulation\n",
    "### Example\n",
    "<img src=\"../images/power-transformation-example.png\" align=\"center\" >\n",
    "\n",
    "## Piecewise linear transformation\n",
    "Complementary to other transformation methods\n",
    "Enable more fine-tuned design of transformations\n",
    "Can have very complex shapes, required more user input.\n",
    "\n",
    "## Piecewise contrast stretching\n",
    "One of the simplest piecewise linear transformations\n",
    "Increase the dynamic range of gray levels in images\n",
    "Used in display devices or recording media to span full range\n",
    "<img src=\"../images/piecewise-contrast-stretching.png\" align=\"center\" >\n",
    "\n",
    "## Gray-level slicing\n",
    "Used to highlight a specific range of gray levels\n",
    "Two different slicing approaches:\n",
    "1. High value of all gray levels in a range of interest and low value for all other (produces binary image)\n",
    "2. Brighten a desired range of gray levels while preserving background and other gray-scale tones of the image.\n",
    "\n",
    "## Bit-plane slicing\n",
    "Highlights contribution to total image by specific bits\n",
    "An image with $n$ bits/pixel has $n$ bit-planes\n",
    "Can be useful for image compression\n",
    "<img src=\"../images/bit-plane-slicing.png\" align=\"center\">\n",
    "<img src=\"../images/bit-plane-slicing-2.png\" align=\"center\">\n",
    "\n",
    "## Histogram of pixel values\n",
    "For every possible gray-level value, count the number of pixels having that value, and plot the pixel counts as a function of gray level.\n",
    "<img src=\"../images/histogram-of-pixel-values.png\" align=\"center\" >\n",
    "\n",
    "## Histogram based thresholding\n",
    "Triangle method for computing threshold automatically\n",
    "1. Find histogram peak $(r_p, h_p)$ and highest gray level point $(r_m,h_m)$\n",
    "2. Construct a straight line $l(r)$ from the peak to the highest gray level point\n",
    "3. FInd the gray level $r$ for which the distance $||l(r) - h(r)||$ is the largest.\n",
    "<img src=\"../images/histogram-based-thresholding.png\" align=\"center\" >\n",
    "\n",
    "\n",
    "# Comparison of thresholding methods\n",
    "<img src=\"../images/Comparison-of-thresholding-methods.png\" align=\"center\" >\n",
    "\n",
    "## Histogram processing\n",
    "\n",
    "### Histogram equalisation\n",
    "**AIM**: To get an image with equality distributed intensity levels over the full intensity range\n",
    "\n",
    "Enhances contrast by redistributing intensity values to make the histogram more uniform, making details in low-contrast areas more visible.\n",
    "\n",
    "<img src=\"../images/Histogram-equalisation.png\" align=\"center\" >\n",
    "\n",
    "Let $r \\in [0, L-1]$ represent pixel values (intensities, gray levels), $r=0$ represents black and $r = L -1$ represents white.\n",
    "\n",
    "Consider transformation $s = T(r), 0 <= r <= L -1$, satisfying\n",
    "1. $T(r)$ is single-valued and monotonically increasing in $) <= r <= L-1$. This guarantees that the inverse transformation $T^{-1}(s)$ exists.\n",
    "2. $0<=T(r)<=L-1$ for $0 <=r<=L-1$. This guarantees that the input and output ranges will be the same.\n",
    "\n",
    "### Histogram specification (histogram matching)\n",
    "**AIM**: To get an image with specified intensity distribution, determined by the shape of the histogram\n",
    "\n",
    "#### Histogram matching examples\n",
    "<img src=\"../images/Histogram-matching-examples.png\" align=\"center\" >\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5020851-ff83-4af0-82de-52cd454855a5",
   "metadata": {},
   "source": [
    "## Arithmetic and logical operations\n",
    "\n",
    "Defined on a pixel-by-pixel between two images\n",
    "<img src=\"../images/Arithmetic-and-logical-operations.png\" align=\"center\">\n",
    "\n",
    "### Addition and subtraction\n",
    "<img src=\"../images/Addition-and-subtraction.png\" align=\"center\">\n",
    "\n",
    "### bitwise AND and OR\n",
    "<img src=\"../images/bitwise-AND-and-OR.png\" align=\"center\">\n",
    "<img src=\"../images/bitwise-AND-and-OR-2.png\" align=\"center\">\n",
    "\n",
    "## Averaging\n",
    "Useful for example to reduce noise in images\n",
    "Assume the true noise-free image is $g(x,y)$ and the actual observed images are $f_i(x,y) = g(x,y) + n_i(x,y)$ for $i = 1,...,N$, where the $n_i$ are zero-mean, independent and identically distributed (i.i.d.) noise images, then we have $E[f_i(x,y)] = g(x,y)$ and $VAR[f_i(x,y)] = VAR[n_i(x,y)] = \\sigma^2(x,y)$\n",
    "$$\\bar{f}(x,y) = \\frac{1}{N}\\sum_{i=1}^Nf_i(x,y)=\\frac{1}{N}\\sum_{i=1}^N[g(x,y)+n_i(x,y)]=g(x,y)+\\frac{1}{N}\\sum_{i=1}^N n_i(x,y)$$\n",
    "$$VAR[\\frac{1}{N}\\sum_{i=1}^Nn_i(x,y)]=\\frac{1}{N^2}\\sum_{i=1}^NVAR[n_i(x,y)]=\\frac{1}{N^2}N\\sigma^2(x,y)=\\frac{\\sigma^2(x,y)}{N}$$\n",
    "\n",
    "Useful for example tor educe noise in images\n",
    "<img src=\"../images/averaging.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a0928b4-ac41-47a1-b205-b21606859d60",
   "metadata": {},
   "source": [
    "## Spatial filtering on groups of pixels\n",
    "\n",
    "Use the gray values in a small *neighbourhood* of a pixel in the output image to produce a new gray value for that pixel in the output image, also called **filtering** techniques because, depending on the weights applied to the pixel values, they suppress (filter out) or enhance information.\n",
    "Neighbourhood of $(x,y)$ is usually a square or rectangular subimage centred at $(x,y)$ and the weights matrix is called the *filter or kernel*\n",
    "Typical kernel sizes are 3 x 3, 5 x 5, 7 x 7 pixels, but can be larger and have different shapes (e.g. circular instead of rect)\n",
    "\n",
    "### Spatial filtering by convolution\n",
    "Output image $o(x,y)$ computed by *discrete convolution* of the given input image $f(x,y)$ and kernel $h(x,y)$\n",
    "<img src=\"../images/Spatial-filtering-by-convolution.png\" align=\"center\">\n",
    "<img src=\"../images/Spatial-filtering-by-convolution-2.png\" align=\"center\">\n",
    "\n",
    "### Equivalent notations of convolution\n",
    "$$o(x,y) = \\sum_{i=-\\infty}^{\\infty} \\sum_{j=-\\infty}^{\\infty} f(x-i, y-j) h(i,j)$$\n",
    "Image coordinates are flipped, out-of-bounds values of $f$ and $h$ are 0.\n",
    "$$o(x,y) = \\sum_{k=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} f(x-k, y-l) h(-k,-l)$$\n",
    "Kernel coordinates are flipped, substitution of variables $k = -1 \\text{ and  } l = -j$\n",
    "$$o(x,y) = \\sum_{k=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} f(k, l) h(x-k,y-l)$$\n",
    "Kernel is shifted and flipped, substitution of variables $k = x-i \\text{ and } l = y - i$\n",
    "\n",
    "### Why must kernel be flipped\n",
    "Because by definition the kernel is the impulse response of the convolution\n",
    "Define input $f$ to be impulse image: $f(0,0) = 1 \\text{ and } f(x,y) = 0$ otherwise\n",
    "Convolution: $o(x,y) = \\sum_{k=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} f(x+k, y+l) h(-k,-l) = h(x,y)$\n",
    "Output is the kernel\n",
    "Correlation: $o(x,y) = \\sum_{k=-\\infty}^{\\infty} \\sum_{l=-\\infty}^{\\infty} f(x+k, y+l) h(k,l) = h(-x,-y)$\n",
    "Output is the flipped kernel\n",
    "\n",
    "### Fixing the border problem\n",
    "Expand the image outside the original border using\n",
    "- Padding: set all additional pixels to a constant (zero) value\n",
    "  Hard transitions yield border artifacts (requires windowing)\n",
    "- Clamping: Repeat all border pixel values indefinitely\n",
    "  Better border behaviour but arbitrary (no theoretical foundation)\n",
    "- Wrapping: Copy pixel values from opposite sides\n",
    "  Implicitly used in the (fast) Fourier transform\n",
    "- Mirroring: Reflect pixel values across border\n",
    "  Smooth, symmetric, periodic, no boundary artifacts\n",
    "<img src=\"../images/Fixing-the-border-problem.png\" align=\"center\">\n",
    "\n",
    "Convolution is a *linear, shift-invariant operation*\n",
    "**Linearity**: If input $f_1(x,y)$ yields ouput $g_1(x,y)$ and $f_2(x,y)$ yields $g_2(x,y)$, then a linear combination of inputs $a_1f_1(x,y) + a_2f_2(x,y)$ yields the same combination of outputs $a_1g_1(x,y) + a_2g_2(x,y)$, for any constants $a_1,a_2$.\n",
    "**Shift invariance**: If input $f(x,y)$ yields output $g(x,y)$ then the shifted input $f(x - \\triangle x,y - \\triangle y)$ yields the shifted output $g(x- \\triangle x, y - \\triangle y)$, in other words, the operation does not discriminate between spatial positions.\n",
    "\n",
    "### Properties of convolution\n",
    "For any set of images (functions) $f_i$ the convolution operation $*$ satisfies:\n",
    "- Commutativity: $f_1 * f_2 = f_2 * f_1$\n",
    "- Associativity: $f_1 * (f_2 * f_3) = (f_1 * f_2) * f_3$\n",
    "- Distributivity\n",
    "- Derivation\n",
    "- Theorem: $f_1 * f_2 <-> \\hat{f_1} \\cdot \\hat{f_2}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c954f0c-c440-4ef9-9979-d785aa0a6cbb",
   "metadata": {},
   "source": [
    "## Simplest smoothing filter\n",
    "\n",
    "Calculates the *mean pixel value* in a neighbourhood $N$ with $|N|$ pixels\n",
    "$$g(x,y) = \\frac{1}{|N|} \\sum \\sum_{(i,j) \\in N} f(x+i, y+j)$$\n",
    "Used for **image blurring** and **noise reduction**\n",
    "Reduces fluctuations due to disturbances in image acquisition\n",
    "Neighbourhood averaging also blurs the object edges in the image\n",
    "Can use weighted average to give more importance to some pixels\n",
    "\n",
    "Also called **uniform filter** as it implicitly uses a uniform kernel.\n",
    "\n",
    "<img src=\"../images/Simplest-smoothing-filter.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee27cec-fd40-406a-abb0-bceb26a07c57",
   "metadata": {},
   "source": [
    "## Gaussian filter\n",
    "One of the most importance basic image filters\n",
    "\n",
    "<img src=\"../images/Gaussian-filter.png\" align=\"center\">\n",
    "<img src=\"../images/Gaussian-filter-2.png\" align=\"center\">\n",
    "\n",
    "Only filter that is both separable and circularly symmetric\n",
    "Fourier transform of a Gaussian is also a Gaussian function\n",
    "Has optimal joint localisation in spacial and frequency domain\n",
    "The n-fold convolution of any low-pass filter converges to a Gaussian\n",
    "Infinitely smooth so it can be differentiated to any desired degree\n",
    "Scales naturally (sigma) and allows for consistent scale-space theory.\n",
    "\n",
    "## Gaussian filter Example\n",
    "<img src=\"../images/Gaussian-filter-Example.png\" align=\"center\" >\n",
    "<img src=\"../images/Gaussian-filter-Example-2.png\" align=\"center\" >"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84d6014-82f5-42cd-a3b1-69f7d41cb651",
   "metadata": {},
   "source": [
    "## Median filter\n",
    "Order-statistics filter (based on ordering and ranking pixel values)\n",
    "Calculates the *median pixel value* in a neighbourhood $N$ with $|N|$ pixels\n",
    "Median value $m$ of a set of ordered values is the **middle value**\n",
    "At most half the values in the set are $< m$ and the other half $> m$\n",
    "\n",
    "Set: ${115, 10, 25, 12, 221, 46, 91, 178, 193}$ <br>\n",
    "Ordered: ${10,12,25,46,91,115,178,193,221}$\n",
    "Median is $91$\n",
    "\n",
    "In case of an event number of values, often the median is taken to be the arithmetic mean of the two middle values.\n",
    "<img src=\"../images/Median-filter.png\" align=\"center\">\n",
    "Taking the minimum or maximum instead of the median is called *min-filtering* and *max-filtering* respectively.\n",
    "\n",
    "Forces pixels with distinct intensities to be more like their neighbours\n",
    "Eliminates isolated intensity spikes (salt and pepper image noise)\n",
    "Neighbourhood is typically of size $n \\times n$ pixels with $n = 3,5,7...$\n",
    "This also eliminates **pixel clusters** (light or dark) with area $< n^2/2$\n",
    "Not a convolution filter but an example of a **nonlinear filter**\n",
    "\n",
    "### Median filtering example\n",
    "<img src=\"../images/Median-filtering-example.png\" align=\"center\" >\n",
    "\n",
    "## Gaussian vs median filtering\n",
    "<img src=\"../images/Gaussian-vs-median-filtering.png\" align=\"center\">\n",
    "\n",
    "## Sharpening by unsharp masking\n",
    "<img src=\"../images/Sharpening-by-unsharp-masking.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd60d946-67e3-4d1f-b41f-e29d39d635c0",
   "metadata": {},
   "source": [
    "## Pooling\n",
    "\n",
    "Combines filtering and downsampling in one operation\n",
    "Examples include min/max/median average pooling\n",
    "Makes the image smaller and reduces computations\n",
    "Popular in deep convolutional neural networks\n",
    "\n",
    "## Derivative filters\n",
    "Spatial derivatives respond to intensity changes (e.g., edges)\n",
    "In digital images they are approximated using finite differences\n",
    "Different possible ways to take finite differences\n",
    "<img src=\"../images/Derivative-filters.png\" align=\"center\">\n",
    "\n",
    "Second order spatial derivative using finite differences\n",
    "<img src=\"../images/Derivative-filters-2.png\" align=\"center\">\n",
    "\n",
    "Sampled approximations of the continuous Gaussian derivatives\n",
    "<img src=\"../images/Derivative-filters-3.png\" align=\"center\">\n",
    "\n",
    "### Gaussian derivative filters\n",
    "Extension of Gaussian derivative kernels to 2D and different spatial scales.\n",
    "<img src=\"../images/Gaussian-derivative-filters.png\" align=\"center\">\n",
    "\n",
    "## Prewitt and Sobel kernels\n",
    "Differentiation in one dimension and smoothing in the other dimension\n",
    "<img src=\"../images/Prewitt-and-Sobel-kernels.png\" align=\"center\">\n",
    "\n",
    "### Separable filter kernels\n",
    "<img src=\"../images/Separable-filter-kernels.png\" align=\"center\">\n",
    "\n",
    "Allow for a much more computationally efficient implementation\n",
    "<img src=\"../images/Separable-filter-kernels-2.png\" align=\"center\">\n",
    "\n",
    "### Example of Prewitt filtering\n",
    "<img src=\"../images/Example-of-Prewitt-filtering.png\" align=\"center\">\n",
    "\n",
    "## Laplacian filtering\n",
    "Approximating the sum of second-order derivatives.\n",
    "<img src=\"../images/Laplacian-filtering.png\" align=\"center\">\n",
    "\n",
    "### Sharpening using the Laplacian\n",
    "<img src=\"../images/Sharpening-using-the-Laplacian.png\" align=\"center\">\n",
    "\n",
    "## Intensity gradient vector\n",
    "Gradient vector (2D)\n",
    "$\\triangledown f(x,y) = [f_x(x,y), f_y(x,y)]^T$\n",
    "\n",
    "Points in the direction of the steepest intensity increase\n",
    "\n",
    "Is orthogonal to isophotes (lines of equal intensity\n",
    "\n",
    "Gradient magnitude (2D)\n",
    "$||\\triangledown f(x,y)|| \\sqrt{f_x^2(x,y) + f_y^2(x,y)}$\n",
    "\n",
    "Represents the length of the gradient vector\n",
    "\n",
    "Is the magnitude of the local intensity change\n",
    "\n",
    "### Edge detection with gradient magnitude\n",
    "<img src=\"../images/Edge-detection-with-gradient-magnitude.png\" align=\"center\">\n",
    "\n",
    "### Edge detection with the Laplacian\n",
    "<img src=\"../images/Edge-detection-the-Laplacian.png\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a80f535-39a1-4edc-9c94-00e43c5c3e84",
   "metadata": {},
   "source": [
    "## Selecting the right spatial scale\n",
    "Computing the image derivatives using Gaussian derivative kernels\n",
    "<img src=\"../images/Selecting-the-right-spatial-scale.png\" align=\"center\" >"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
